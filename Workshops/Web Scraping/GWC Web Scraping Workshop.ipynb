{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Girls Who Code - The Python Series\n",
    "## Web Scraping\n",
    "## Mentor - Amir ElTabakh\n",
    "\n",
    "\n",
    "**Welcome** to the third Python workshop of the Fall 21 semester! Today we're going to learn about Web Scraping.\n",
    "\n",
    "Web scraping is the process of extracting data from websites. For our webscrape, we will use Chrome Developer Tools to identify HTML components. We will also use the Python BeautifulSoup and Splinter libraries to automate a web browser and gather the data we've identified.\n",
    "\n",
    "Web scraping is a method of collecting data from different web sources quickly instead of manually visiting each one, which can be time consuming. This is our first taste in using Python to really automate a process. There are many steps to collecting data from the web, which we'll go over today.\n",
    "\n",
    "In this workshop we will go over:\n",
    "- HTML code and how websites are structured\n",
    "- Write a Python script that automates exploring the web with Splinter\n",
    "- Collect data from a website using BeautifulSoup\n",
    "\n",
    "Splinter is the tool that automates a web browser. It's pretty cool to see your computer navigate the web all on its own.\n",
    "BeautifulSoup will extract the data needed for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "HTML is a coding language used for creating webpages. It’s built using specific tags and arranging them in a nested order, a bit like building blocks. For example, if we wanted a header and a paragraph in the same section of a webpage, we would nest `<h1 />` and `<p />` tags inside a `<div />` tag, with the `<div />` tag acting as a box to hold the other pieces.\n",
    "\n",
    "```\n",
    "<div>\n",
    "   <h1>Hello, world!</h1>\n",
    "   <p>This is a great beginning.</p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Think of a webpage as a window into the internet. HTML is the glass, boards, and blinds on that window. Just like there are many sizes and shapes to windows, each webpage has been customized to present users with a view into a different topic. Consider a weather report delivered through a weather site. Think of a news source or social media platform. Each of these examples are all built using custom HTML. Our first step will be to explore that design so that we can write a script that knows what it's looking at when it interacts with a webpage.\n",
    "\n",
    "Open VS Code and create a file named index.html. This file can be saved to your desktop because it's just for practice.\n",
    "\n",
    "In this blank HTML file put an exclamation point on the first line and press Enter. This should autofill the editor to contain everything we need for a basic HTML page.\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    " <meta charset=\"UTF-8\">\n",
    " <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    " <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n",
    " <title>Document</title>\n",
    "</head>\n",
    "<body>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Most elements have opening and closing tags, which are identical except for the forward slash that begins the closing tag. The closing tags represent the end of that HTML element.\n",
    "\n",
    "Let's define each HTML tag shown in the graphic:\n",
    "\n",
    "1. `<!DOCTYPE html>` is a declaration, not a tag. It tells web browsers in which HTML version the document is written. This should always be the first line in an HTML document.\n",
    "\n",
    "2. `<head>` is the opening tag that serves as a container for the setup elements. Jupyter Notebook imports occur in the top cell whereas Python imports occur at the top of the code. HTML imports (e.g., a stylesheet or a library) will be within the `<head>`.\n",
    "\n",
    "3. `<meta>` is short for \"metadata\" and tells the web browser basic information, such as page width.\n",
    "\n",
    "4. `<title>` and `</title>` are the opening and closing tags that serve as a container for the page title displayed on the tab at the top of your web browser. In the example above, the title is \"Document\" and would appear like so in the browser:\n",
    "\n",
    "5. `</head>` is the closing tag for the `<head>` tag, much like the end of a code block in Python.\n",
    "\n",
    "6. `<body>` and `</body>` are opening and closing tags. They also serve as a container, but for data we can see (navigation menus, lists, and paragraphs).\n",
    "\n",
    "7. `<html lang=”en”>` and `</html>` are opening and closing tags that serve as a container for all elements within an HTML page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to keep the tags in visual order is by using indentation. Containers nested within other containers are indented by two to four spaces. This helps to keep our code clean and easy to understand. Nesting is when HTML elements are contained within other elements. Picture a set of nesting dolls with each nested in proper order, by design, into the largest doll. It is the same for HTML tags—they must be in the correct order to not break the design of the webpage.\n",
    "\n",
    "Let's take another look at this webpage, only with a few more elements added to it:\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    " <head>\n",
    "   <meta charset=\"UTF-8\" />\n",
    "   <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "   <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" />\n",
    "   <title>Document</title>\n",
    " </head>\n",
    " <body>\n",
    "   <h1>Hello, world!</h1>\n",
    "   <p>\n",
    "     Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin aliquet\n",
    "     iaculis lorem non sollicitudin. Fusce elementum ac elit finibus auctor.\n",
    "     Curabitur orci sem, accumsan a diam sit amet, efficitur tristique velit.\n",
    "   </p>\n",
    "   <ul>\n",
    "     <li>First list item</li>\n",
    "     <li>Second list item</li>\n",
    "     <li>Third list item</li>\n",
    "   </ul>\n",
    " </body>\n",
    "</html>\n",
    "```\n",
    "There are several more tags within the <body /> container. Add this new code to your index.html file and save it. Then, open the file by navigating to it and double-clicking it. Now you have a simple static webpage open in your browser, built from scratch. It's not super exciting yet, but that's okay. It's the innards of the page we're focusing on right now.\n",
    "\n",
    "Let's review the new tags:\n",
    "\n",
    "1. `<h1 />` is a first-level header. The text in this tag will be displayed bigger and bolder than the rest of the page's text. There are many different headers available to use, from h1 to h6, with h1 returning the largest text.\n",
    "2. `<p />` is a paragraph tag, currently holding lorem ipsum sentences. (lorem ipsum is dummy text used to stage websites). More can be read about it on the Lorem Ipsum reference website (Links to an external site.).\n",
    "3. `<ul />` is an unordered list.\n",
    "4. `<li />` is a list item.\n",
    "\n",
    "_Question_: What does it mean when the `<li />` tags are inside the `<ul />` tags?\n",
    "\n",
    "This is only a small taste of how many tags exist out there. Remember, these tags are all part of website customization. Without the variety available to use, websites would look plain and uninspired. The sites that Robin intends to scrape data from are far more sophisticated, using many more combinations of tags than what we've discussed here. Understanding the basic layout and how nesting and containers work is an important part of successful web scraping.\n",
    "\n",
    "We know that when we scrape data from the web, we're simply pulling specific data from websites we've chosen. How do we specify the data? Let's say we want the latest news article from a Mars website. Before we can program our script to pull that data, we have to tell it where to look. Basically, our script would say, \"look in this `<div />` tag, then look inside that for a `<p />` tag.\"\n",
    "\n",
    "That's a simple way of putting it, visit W3Schools' developer site for an extensive list of [HTML tags](https://www.w3schools.com/tags/tag_comment.asp).\n",
    "\n",
    "## Splinter\n",
    "\n",
    "One of the fun things about web scraping is the automation—watching your script at work.\n",
    "\n",
    "1. Once you execute your completed scraping script, a new Chrome web browser will pop up with a banner across the top that says \"Chrome is being controlled by automated test software.\"\n",
    "2. This message lets you know that your Python script is directing the browser. The browser will visit websites and interact with them on its own.\n",
    "3. Depending on how you've programmed your script, your browser will click buttons, use a search bar, or even log in to a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install splinter\n",
    "!pip install bs4\n",
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac users use this block of code\n",
    "\n",
    "# Set the executable path and initialize the chrome browser in splinter\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 95.0.4638\n",
      "[WDM] - Get LATEST driver version for 95.0.4638\n",
      "[WDM] - Driver [C:\\Users\\amira\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Windows users use this block of code\n",
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An HTML page can get very confusing very quickly, so lets practice on a less sophisticated site first. There are several sites available specifically for newly minted web scrapers to practice and hone their skills with Splinter and BeautifulSoup. These practice sites contain several different components that we'll encounter out in the wild: buttons to navigate, search bars, and nested HTML tags. It's a great introduction to how the tools we'll use work together to gather the data we want.\n",
    "\n",
    "Lets scrape data from a website specifically created for practicing web scraping! Head to [Quotes to Scrape](https://quotes.toscrape.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 95.0.4638\n",
      "[WDM] - Get LATEST driver version for 95.0.4638\n",
      "[WDM] - Driver [C:\\Users\\amira\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Top Ten tags\n"
     ]
    }
   ],
   "source": [
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Visit the Quotes to Scrape site\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Parse the HTML\n",
    "html = browser.html\n",
    "html_soup = soup(html, 'html.parser')\n",
    "\n",
    "# Scrape the Title\n",
    "title = html_soup.find('h2').text\n",
    "print(\"Title: \" + title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've just done in the last two lines of code is:\n",
    "\n",
    "We used our html_soup object we created earlier and chained find() to it to search for the `<h2 />` tag.\n",
    "We've also extracted only the text within the HTML tags by adding `.text` to the end of the code.\n",
    "We've completed our first actual scrape. Let's practice again, this time using Splinter to scrape the actual tags to go with the title we just pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 95.0.4638\n",
      "[WDM] - Get LATEST driver version for 95.0.4638\n",
      "[WDM] - Driver [C:\\Users\\amira\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "inspirational\n",
      "life\n",
      "humor\n",
      "books\n",
      "reading\n",
      "friendship\n",
      "friends\n",
      "truth\n",
      "simile\n"
     ]
    }
   ],
   "source": [
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Visit the Quotes to Scrape site\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Scrape the top ten tags\n",
    "tag_box = html_soup.find('div', class_='tags-box')\n",
    "# tag_box\n",
    "tags = tag_box.find_all('a', class_='tag')\n",
    "\n",
    "for tag in tags:\n",
    "    word = tag.text\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code looks really similar to our last, but we've increased the difficulty a bit by incorporating a for loop, but let's start at the beginning.\n",
    "\n",
    "The first line, `tag_box = html_soup.find('div', class_='tags-box')`, creates a new variable `tag_box`, which will be used to store the results of a search. In this case, we're looking for `<div />` elements with a class of `tags-box`, and we're searching for it in the HTML we parsed earlier and stored in the html_soup variable.\n",
    "\n",
    "The second line, `tags = tag_box.find_all('a', class_='tag')`, is similar to the first but with a few tweaks to make the search more specific. The new \"tags\" variable will hold the results of a `find_all`, but this time we're searching through the parsed results stored in our `tag_box` variable to find `<a />` elements with a `tag` class.\n",
    "\n",
    "We used `find_all` this time because we want to capture all results, instead of a single or specific one.\n",
    "\n",
    "Next, we've added a for loop. This for loop cycles through each tag in the `tags` variable, strips the HTML code out of it, and then prints only the text of each tag.\n",
    "\n",
    "### Scrape Across Pages\n",
    "Now that we've practiced scraping items from a single page, we're going to up the ante by scraping items that span multiple pages. Our next section of code will scrape the quotes on the first page, click the \"Next\" button, then scrape more quotes and so on (five pages worth of quotes).\n",
    "\n",
    "The first two lines do two things: They assign an actual URL to the variable named \"url\" and then tell Splinter to visit that webpage. We'll create a for loop to collect each quote, \"click\" the next button, then collect the next set of quotes. We'll use `range(1, 6)` in our for loop to visit the first five pages of the website.\n",
    "\n",
    "Go ahead and execute this cell. This will cause the automated browser to navigate there and run our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 95.0.4638\n",
      "[WDM] - Get LATEST driver version for 95.0.4638\n",
      "[WDM] - Driver [C:\\Users\\amira\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 1 ----------\n",
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "page: 1 ----------\n",
      "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "page: 1 ----------\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "page: 1 ----------\n",
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "page: 1 ----------\n",
      "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "page: 1 ----------\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "page: 1 ----------\n",
      "“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "page: 1 ----------\n",
      "“I have not failed. I've just found 10,000 ways that won't work.”\n",
      "page: 1 ----------\n",
      "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "page: 1 ----------\n",
      "“A day without sunshine is like, you know, night.”\n",
      "page: 2 ----------\n",
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "page: 2 ----------\n",
      "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "page: 2 ----------\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "page: 2 ----------\n",
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "page: 2 ----------\n",
      "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "page: 2 ----------\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "page: 2 ----------\n",
      "“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "page: 2 ----------\n",
      "“I have not failed. I've just found 10,000 ways that won't work.”\n",
      "page: 2 ----------\n",
      "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "page: 2 ----------\n",
      "“A day without sunshine is like, you know, night.”\n",
      "page: 3 ----------\n",
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "page: 3 ----------\n",
      "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "page: 3 ----------\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "page: 3 ----------\n",
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "page: 3 ----------\n",
      "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "page: 3 ----------\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "page: 3 ----------\n",
      "“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "page: 3 ----------\n",
      "“I have not failed. I've just found 10,000 ways that won't work.”\n",
      "page: 3 ----------\n",
      "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "page: 3 ----------\n",
      "“A day without sunshine is like, you know, night.”\n",
      "page: 4 ----------\n",
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "page: 4 ----------\n",
      "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "page: 4 ----------\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "page: 4 ----------\n",
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "page: 4 ----------\n",
      "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "page: 4 ----------\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "page: 4 ----------\n",
      "“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "page: 4 ----------\n",
      "“I have not failed. I've just found 10,000 ways that won't work.”\n",
      "page: 4 ----------\n",
      "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "page: 4 ----------\n",
      "“A day without sunshine is like, you know, night.”\n",
      "page: 5 ----------\n",
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "page: 5 ----------\n",
      "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "page: 5 ----------\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "page: 5 ----------\n",
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "page: 5 ----------\n",
      "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "page: 5 ----------\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "page: 5 ----------\n",
      "“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "page: 5 ----------\n",
      "“I have not failed. I've just found 10,000 ways that won't work.”\n",
      "page: 5 ----------\n",
      "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "page: 5 ----------\n",
      "“A day without sunshine is like, you know, night.”\n"
     ]
    }
   ],
   "source": [
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "for x in range(1, 6): # A for loop with five iterations\n",
    "    \n",
    "    html = browser.html # An HTML object assigned to the `html` variable\n",
    "    \n",
    "    quote_soup = soup(html, 'html.parser') # Use BeautifulSoup to parse the `html` object\n",
    "    \n",
    "    quotes = quote_soup.find_all('span', class_='text') # Use BeautifulSoup to find all `<span />` tags with a class of \"text\"\n",
    "    \n",
    "    for quote in quotes: # Print statements wrapped in another for loop\n",
    "        print('page:', x, '----------')\n",
    "        print(quote.text)\n",
    "        \n",
    "    browser.links.find_by_partial_text('Next') # Use Splinter to click the 'Next' button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA has a very friendly Terms of Service (or ToS, also known as Terms of Use) when it comes to web scraping. So lets scrape some data from there! In the next cell of your Jupyter notebook, we'll assign the url and instruct the browser to visit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visit the mars nasa news site\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url)\n",
    "# Optional delay for loading the page\n",
    "browser.is_element_present_by_css(\"ul.item_list li.slide\", wait_time=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following line, `browser.is_element_present_by_css(\"ul.item_list li.slide\", wait_time=1)`, we are accomplishing two things.\n",
    "\n",
    "One is that we're searching for elements with a specific combination of tag (`ul` and `li`) and attribute (`item_list` and `slide`, respectively). For example, `ul.item_list` would be found in HTML as `<ul class=”item_list”>`.\n",
    "\n",
    "Secondly, we're also telling our browser to wait one second before searching for components. The optional delay is useful because sometimes dynamic pages take a little while to load, especially if they are image-heavy.\n",
    "\n",
    "In the next cell, we'll set up the HTML parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "news_soup = soup(html, 'html.parser')\n",
    "slide_elem = news_soup.select_one('ul.item_list li.slide')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we've assigned `slide_elem` as the variable to look for the `<ul />` tag and its descendent (the other tags within the `<ul />` element), the `<li />` tags? This is our parent element. This means that this element holds all of the other elements within it, and we'll reference it when we want to filter search results even further. The `.` is used for selecting classes, such as `item_list`, so the code `'ul.item_list li.slide'` pinpoints the `<li />` tag with the class of slide and the `<ul />` tag with a class of `item_list`. CSS works from right to left, such as returning the last item on the list instead of the first. Because of this, when using select_one, the first matching element returned will be a `<li />` element with a class of slide and all nested elements within it.\n",
    "\n",
    "After opening the page in a new browser, right-click to inspect and activate your DevTools. Then search for the HTML components you'll use to identify the title and paragraph you want.\n",
    "\n",
    "_Question_: Which HTML attribute will we use to scrape the article’s title?\n",
    "\n",
    "\n",
    "There are two methods used to find tags and attributes with BeautifulSoup:\n",
    "- `.find()` is used when we want only the first class and attribute we've specified.\n",
    "- `.find_all()` is used when we want to retrieve all of the tags and attributes.\n",
    "\n",
    "For example, if we were to use `.find_all()` instead of `.find()` when pulling the summary, we would retrieve all of the summaries on the page instead of just the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 95.0.4638\n",
      "[WDM] - Get LATEST driver version for 95.0.4638\n",
      "[WDM] - Driver [C:\\Users\\amira\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<div class=\"content_title\"><a href=\"/news/9063/you-can-help-train-nasas-rovers-to-better-explore-mars/\" target=\"_self\">You Can Help Train NASA's Rovers to Better Explore Mars</a></div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the title\n",
    "\n",
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Visit the mars nasa news site\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Optional delay for loading the page\n",
    "browser.is_element_present_by_css(\"ul.item_list li.slide\", wait_time=1)\n",
    "\n",
    "html = browser.html\n",
    "news_soup = soup(html, 'html.parser')\n",
    "slide_elem = news_soup.select_one('ul.item_list li.slide')\n",
    "\n",
    "slide_elem.find(\"div\", class_='content_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this line of code, we chained `.find` onto our previously assigned variable, `slide_elem`. When we do this, we're saying, \"This variable holds a ton of information, so look inside of that information to find this specific data.\" The data we're looking for is the content title, which we've specified by saying, \"The specific data is in a `<div />` with a class of `'content_title'`.\"\n",
    "\n",
    "The title is in that mix of HTML in our output—that's awesome! But we need to get just the text, and the extra HTML stuff isn't necessary. We'll add something new to our `.find()` method here: `.get_text()`. When this new method is chained onto `.find()`, only the text of the element is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You Can Help Train NASA's Rovers to Better Explore Mars\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the parent element to find the first `a` tag and save it as `news_title`\n",
    "news_title = slide_elem.find(\"div\", class_='content_title').get_text()\n",
    "news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Members of the public can now help teach an artificial intelligence algorithm to recognize scientific features in images taken by NASA’s Perseverance rover.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the parent element to find the paragraph text\n",
    "news_p = slide_elem.find('div', class_=\"article_teaser_body\").get_text()\n",
    "news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! Stretch your scraping skills by visiting [Books to Scrape](http://books.toscrape.com/) and scraping the book URL list on the first page.\n",
    "\n",
    "Many websites don't want automated browsers visiting their sites and snagging data. If there are too many visits, the server hosting the site could get overloaded and shut down. Administrators can then ban the IP address of the person doing the scraping, making it more difficult to even manually visit the site to view data.\n",
    "\n",
    "**IMPORTANT**\\\n",
    "Terms of Service and Terms of Use bring up an ethical issue when gathering data. Many websites don't allow automated browsing and scraping—some of the scraping scripts out there are designed to gather data quickly, and the constant traffic can overload web servers and disable a website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
